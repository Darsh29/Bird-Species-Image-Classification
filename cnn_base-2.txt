# %%
# Import Data Science Libraries
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Tensorflow Libraries
from tensorflow import keras
from tensorflow.keras import layers, models
from keras_preprocessing.image import ImageDataGenerator

from data_generation import prepare_from_combined

BATCH_SIZE = 32
TARGET_SIZE = (224, 224)

# %%
train_df, val_df, test_df = prepare_from_combined()

# %%
train_generator = ImageDataGenerator(
                        rescale=1./255,
                        rotation_range=30,
                        brightness_range=[0.7,1.3],
                        zoom_range=0.3,
                        width_shift_range=0.2, 
                        height_shift_range=0.2,
                        horizontal_flip=True)

test_generator = ImageDataGenerator(rescale=1./255)

# Generate Training images
train_images = train_generator.flow_from_dataframe(
                        dataframe=train_df,
                        x_col='Filepath',
                        y_col='Label',
                        target_size=TARGET_SIZE,  # all images will be resized to TARGET_SIZE
                        interpolation='nearest',
                        color_mode='rgb',
                        class_mode='categorical',
                        classes=list(train_df['Label'].unique()),
                        batch_size=BATCH_SIZE,
                        shuffle=True,
                        seed=175)

# Generate Validation images
val_images = test_generator.flow_from_dataframe(
                    dataframe=val_df,
                    x_col='Filepath',
                    y_col='Label',
                    target_size=TARGET_SIZE,  # all images will be resized to TARGET_SIZE
                    interpolation='nearest',
                    color_mode='rgb',
                    class_mode='categorical',
                    classes=list(train_df['Label'].unique()),
                    batch_size=BATCH_SIZE,
                    shuffle=True,
                    seed=42)

# Generate test images
test_images = test_generator.flow_from_dataframe(
                    dataframe=test_df,
                    x_col='Filepath',
                    y_col='Label',
                    target_size=TARGET_SIZE,  # all images will be resized to TARGET_SIZE
                    interpolation='nearest',
                    color_mode='rgb',
                    class_mode='categorical',
                    classes=list(train_df['Label'].unique()),
                    shuffle=False,
                    seed=42)

# %%
model = models.Sequential()
model.add(layers.Conv2D(16, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)))
model.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model.add(layers.BatchNormalization())

model.add(layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'))
model.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model.add(layers.BatchNormalization())

model.add(layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))
model.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model.add(layers.BatchNormalization())

model.add(layers.Conv2D(128, (5, 5), strides=(1, 1), padding='same', activation='relu'))
model.add(layers.MaxPooling2D(pool_size=3, padding="same"))
model.add(layers.BatchNormalization())

model.add(layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same', activation='relu'))
model.add(layers.MaxPooling2D(pool_size=3, padding="same"))
model.add(layers.BatchNormalization())

# model.add(layers.Conv2D(256, (3, 3), strides=(2, 2), padding='same', activation='relu'))
# model.add(layers.GlobalAveragePooling2D())

# model.add(layers.Dropout(0.25))

model.add(layers.Flatten())
model.add(layers.Dense(32, activation='relu'))
model.add(layers.BatchNormalization())
model.add(layers.Dense(526, activation='softmax'))

opt = keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt,
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

model.summary()

# %%
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="./model/cnn_baseline.keras",
        save_best_only=True,
        monitor="val_loss"), 
    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', 
                                      factor=0.2, 
                                      patience=3, 
                                      min_lr=1e-6),
    keras.callbacks.EarlyStopping(monitor='val_loss', 
                                  verbose=1,
                                  min_delta=0.01,
                                  patience=5)
                                      ]

history = model.fit(train_images, epochs=100, 
                    validation_data=val_images, callbacks=callbacks)
model.save_weights('./model/cnn_baseline.weights.h5') 

# %%
plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
# plt.ylim([0.8, 1])
plt.legend(loc='lower right')

# %%
model_2 = models.Sequential()
model_2.add(layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)))
model_2.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_2.add(layers.BatchNormalization())

model_2.add(layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'))
model_2.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_2.add(layers.BatchNormalization())

model_2.add(layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))
model_2.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_2.add(layers.BatchNormalization())

model_2.add(layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='relu'))
model_2.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_2.add(layers.BatchNormalization())

model_2.add(layers.Conv2D(128, (7, 7), strides=(1, 1), padding='same', activation='relu'))
model_2.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_2.add(layers.BatchNormalization())

# model_2.add(layers.Dropout(0.25))

model_2.add(layers.Flatten())
model_2.add(layers.Dense(32, activation='relu'))
model_2.add(layers.BatchNormalization())
model_2.add(layers.Dense(526, activation='softmax'))

opt = keras.optimizers.Adam(learning_rate=0.001)
model_2.compile(optimizer=opt,
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

model_2.summary()

# %%
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="./model/cnn_baseline_improved.keras",
        save_best_only=True,
        monitor="val_loss"), 
    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', 
                                      factor=0.2, 
                                      patience=3, 
                                      min_lr=1e-6),
    keras.callbacks.EarlyStopping(monitor='val_loss', 
                                  verbose=1,
                                  min_delta=0.01,
                                  patience=5)
                                      ]

history_2 = model_2.fit(train_images, epochs=100, 
                    validation_data=val_images, callbacks=callbacks)
model_2.save_weights('./model/cnn_baseline_improved.weights.h5') 

# %%
plt.plot(history_2.history['accuracy'], label='accuracy')
plt.plot(history_2.history['val_accuracy'], label = 'val_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
# plt.ylim([0.8, 1])
plt.legend(loc='lower right')

# %%
model_3 = models.Sequential()
model_3.add(layers.Conv2D(16, (3, 3), strides=(1, 1), padding='same', activation='relu', input_shape=(TARGET_SIZE[0], TARGET_SIZE[1], 3)))
model_3.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_3.add(layers.BatchNormalization())

model_3.add(layers.Conv2D(32, (3, 3), strides=(1, 1), padding='same', activation='relu'))
model_3.add(layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same', activation='relu'))
model_3.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_3.add(layers.BatchNormalization())

model_3.add(layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='relu'))
model_3.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', activation='relu'))
model_3.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_3.add(layers.BatchNormalization())

model_3.add(layers.Conv2D(128, (7, 7), strides=(1, 1), padding='same', activation='relu'))
model_3.add(layers.Conv2D(128, (7, 7), strides=(2, 2), padding='same', activation='relu'))
model_3.add(layers.MaxPooling2D(pool_size=2, padding="same"))
model_3.add(layers.BatchNormalization())

# model_2.add(layers.Dropout(0.25))

model_3.add(layers.Flatten())
model_3.add(layers.Dense(32, activation='relu'))
model_3.add(layers.BatchNormalization())
model_3.add(layers.Dense(526, activation='softmax'))

opt = keras.optimizers.Adam(learning_rate=0.001)
model_3.compile(optimizer=opt,
              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
              metrics=['accuracy'])

model_3.summary()

# %%
callbacks = [
    keras.callbacks.ModelCheckpoint(
        filepath="./model/cnn_baseline_improved.keras",
        save_best_only=True,
        monitor="val_loss"), 
    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', 
                                      factor=0.75, 
                                      patience=3, 
                                      min_lr=1e-6),
    keras.callbacks.EarlyStopping(monitor='val_loss', 
                                  verbose=1,
                                  min_delta=0.01,
                                  patience=5)
                                      ]

history_3 = model_3.fit(train_images, epochs=100, 
                    validation_data=val_images, callbacks=callbacks)
model_3.save_weights('./model/cnn_baseline_improved.weights.h5') 

# %%



